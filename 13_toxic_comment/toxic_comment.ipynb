{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-и-изучение\" data-toc-modified-id=\"Загрузка-и-изучение-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Загрузка и изучение</a></span></li><li><span><a href=\"#Предварительная-обработка-данных\" data-toc-modified-id=\"Предварительная-обработка-данных-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Предварительная обработка данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Очистка-данных,-Токенизация,-Лемматизация\" data-toc-modified-id=\"Очистка-данных,-Токенизация,-Лемматизация-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Очистка данных, Токенизация, Лемматизация</a></span><ul class=\"toc-item\"><li><span><a href=\"#WordNetLemmatizer\" data-toc-modified-id=\"WordNetLemmatizer-1.2.1.1\"><span class=\"toc-item-num\">1.2.1.1&nbsp;&nbsp;</span>WordNetLemmatizer</a></span></li></ul></li><li><span><a href=\"#Векторизация\" data-toc-modified-id=\"Векторизация-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Векторизация</a></span><ul class=\"toc-item\"><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-1.2.2.1\"><span class=\"toc-item-num\">1.2.2.1&nbsp;&nbsp;</span>TF-IDF</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#LogisticRegression-+-TF-IDF\" data-toc-modified-id=\"LogisticRegression-+-TF-IDF-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>LogisticRegression + TF-IDF</a></span></li><li><span><a href=\"#LinearSVC\" data-toc-modified-id=\"LinearSVC-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>LinearSVC</a></span></li><li><span><a href=\"#CatBoost-на-неподготовленных-данных\" data-toc-modified-id=\"CatBoost-на-неподготовленных-данных-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>CatBoost на неподготовленных данных</a></span></li><li><span><a href=\"#LGBM\" data-toc-modified-id=\"LGBM-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>LGBM</a></span></li></ul></li><li><span><a href=\"#Выбор-модели\" data-toc-modified-id=\"Выбор-модели-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выбор модели</a></span></li><li><span><a href=\"#Тестирование\" data-toc-modified-id=\"Тестирование-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Тестирование</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from time import time\n",
    "from scipy.sparse import vstack\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", \n",
    "                    datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from catboost import CatBoostClassifier, cv, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from tqdm import notebook\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import (cross_val_score, \n",
    "                                     train_test_split, \n",
    "                                     GridSearchCV,\n",
    "                                     StratifiedKFold,\n",
    "                                     RepeatedStratifiedKFold,\n",
    "                                     RandomizedSearchCV\n",
    "                                    )\n",
    " \n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "from pprint import pprint\n",
    "\n",
    "N_JOBS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и изучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_read(df_name):\n",
    "\n",
    "    data = pd.read_csv(f'C:\\\\Users\\\\m5612\\\\Downloads\\\\{df_name}.csv', \n",
    "    index_col=[0])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data_read('toxic_comments')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>159292.0</td>\n",
       "      <td>0.101612</td>\n",
       "      <td>0.302139</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count      mean       std  min  25%  50%  75%  max\n",
       "toxic  159292.0  0.101612  0.302139  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пропусков в данных нет, есть дисбаланс по целевому признаку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительная обработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом этапе данные очищаются от нежелательных символов, приводятся к нижнему регистру и токенизируются (разбиваются на отдельные слова). Кроме того, может проводиться удаление стоп-слов (часто встречающихся слов, которые не несут информации) и лемматизация (приведение слов к их базовой форме)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Очистка данных, Токенизация, Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стоп-слова\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем объект для лемматизации слов\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для очистки и предварительной обработки текста\n",
    "def preprocess_text(text):\n",
    "    # удаляем все символы, кроме букв и цифр\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    # приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # токенизируем текст\n",
    "    words = word_tokenize(text)\n",
    "    # лемматизируем слова\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    # удаляем стоп-слова\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    # объединяем слова в строку\n",
    "    text = ' '.join(words)\n",
    "    if len(text) > 1:\n",
    "        return text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [20:31<00:00, 129.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18min 21s, sys: 1min 49s, total: 20min 11s\n",
      "Wall time: 20min 31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data['text'] = data['text'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'explanation edits make username hardcore metallica fan revert vandalism closure gas vote new york doll fac please remove template talk page since retire 89 205 38 27',\n",
       " 1: 'aww match background colour seemingly stuck thanks talk 21 51 january 11 2016 utc',\n",
       " 2: 'hey man really try edit war guy constantly remove relevant information talk edits instead talk page seem care format actual info',\n",
       " 3: 'make real suggestion improvement wonder section statistic later subsection type accident think reference may need tidy exact format ie date format etc later one else first preference format style reference want please let know appear backlog article review guess may delay reviewer turn list relevant form eg wikipedia good_article_nominations transport',\n",
       " 4: 'sir hero chance remember page'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'].head(5).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этап векторизации включает преобразование текстовых данных в числовые векторы, которые могут быть использованы для обучения моделей машинного обучения.\n",
    "\n",
    "Существует несколько способов векторизации текстовых данных, но наиболее распространенными являются:\n",
    "\n",
    "- Мешок слов (bag-of-words): Это простой метод, который представляет каждый документ (или предложение) как набор слов без учета порядка слов в тексте. Сначала составляется словарь всех уникальных слов в корпусе текстовых данных, а затем каждый документ представляется в виде вектора, где каждый элемент соответствует слову в словаре, а значение элемента указывает на количество вхождений этого слова в документ. Векторы полученные с помощью мешка слов могут быть очень большими, но они могут быть уменьшены с помощью методов сокращения размерности, таких как сингулярное разложение (SVD) или метод главных компонент (PCA).\n",
    "\n",
    "- TF-IDF (Term Frequency - Inverse Document Frequency): Это более продвинутый метод векторизации, который также использует мешок слов. Он учитывает не только частоту слова в документе, но и его важность в корпусе текстовых данных. Таким образом, слова, которые часто встречаются в документе, но редко в других документах, имеют более высокий вес.\n",
    "\n",
    "- Word2Vec: Это метод, который использует нейронные сети для преобразования слов в векторы фиксированной длины. Каждое слово представляется в виде вектора, который учитывает контекст, в котором оно находится. Это позволяет сохранять смысловые отношения между словами в векторах. Word2Vec может быть обучен на больших наборах текстовых данных, что позволяет получить более точные представления слов.\n",
    "\n",
    "- BERT (Bidirectional Encoder Representations from Transformers): Это метод, который использует глубокие нейронные сети, основанные на архитектуре Transformer, для получения представлений текстовых данных. Он учитывает контекст и порядок слов в предложении, что позволяет ему получать более точные представления. BERT был обучен на огромных наборах текстовых данных и может быть использован для решения различных задач, таких как классификация текста, вопросно-ответные системы и машинный перевод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(data['text'], \n",
    "                                                                data['toxic'], \n",
    "                                                                test_size=0.1, \n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, \n",
    "                                                      y_train_valid, \n",
    "                                                      test_size=0.1, \n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер X_train_valid: (143336,)\n",
      "Размер y_train_valid: (143336,)\n",
      "Размер X_train: (129002,)\n",
      "Размер X_test: (15927,)\n",
      "Размер y_train: (129002,)\n",
      "Размер y_test: (15927,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер X_train_valid:\", X_train_valid.shape)\n",
    "print(\"Размер y_train_valid:\", y_train_valid.shape)\n",
    "print(\"Размер X_train:\", X_train.shape)\n",
    "print(\"Размер X_test:\", X_test.shape)\n",
    "print(\"Размер y_train:\", y_train.shape)\n",
    "print(\"Размер y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы X_train_tfidf: (129002, 151004)\n",
      "Размер матрицы X_test_tfidf: (15927, 151004)\n",
      "Размер матрицы X_valid_tfidf: (14334, 151004)\n",
      "Размер матрицы X_train_valid_tfidf: (143336, 151004)\n",
      "CPU times: user 9.08 s, sys: 189 ms, total: 9.26 s\n",
      "Wall time: 9.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_tf_idf = TfidfVectorizer() \n",
    "X_train_tfidf = count_tf_idf.fit_transform(X_train) \n",
    "X_test_tfidf = count_tf_idf.transform(X_test)\n",
    "X_valid_tfidf = count_tf_idf.transform(X_valid)\n",
    "X_train_valid_tfidf = count_tf_idf.transform(X_train_valid)\n",
    "print(\"Размер матрицы X_train_tfidf:\", X_train_tfidf.shape)\n",
    "print(\"Размер матрицы X_test_tfidf:\", X_test_tfidf.shape)\n",
    "print(\"Размер матрицы X_valid_tfidf:\", X_valid_tfidf.shape)\n",
    "print(\"Размер матрицы X_train_valid_tfidf:\", X_train_valid_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<129002x151004 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3486364 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                ('model_lr', LogisticRegression())])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TfidfVectorizer()),\n",
    "        (\"model_lr\", LogisticRegression())\n",
    "    ]\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Hyperparameters to be evaluated:\n",
      "{'model_lr__C': [0.1, 1, 10],\n",
      " 'model_lr__solver': ['saga'],\n",
      " 'vect__max_df': (0.6, 0.8, 1.0),\n",
      " 'vect__min_df': (1, 3, 5),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2)),\n",
      " 'vect__norm': ('l1', 'l2')}\n"
     ]
    }
   ],
   "source": [
    "# Задание сетки параметров\n",
    "parameter_grid = {\n",
    "    \"vect__max_df\": (0.4, 0.6, 0.8, 1.0),\n",
    "    \"vect__min_df\": (1, 3, 5),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    \"vect__norm\": (\"l1\", \"l2\"),\n",
    "    'model_lr__C': [0.1, 1, 10],\n",
    "    'model_lr__solver': ['saga']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=parameter_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_iter=12,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"Hyperparameters to be evaluated:\")\n",
    "pprint(parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Done in 834.445s\n",
      "CPU times: user 13min 31s, sys: 21.8 s, total: 13min 53s\n",
      "Wall time: 13min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = time()\n",
    "random_search.fit(X_train_valid, y_train_valid)\n",
    "print(f\"Done in {time() - t0:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters combination found:\n",
      "model_lr__C: 10\n",
      "model_lr__solver: saga\n",
      "vect__max_df: 0.6\n",
      "vect__min_df: 3\n",
      "vect__ngram_range: (1, 1)\n",
      "vect__norm: l2\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters combination found:\")\n",
    "best_parameters = random_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameter_grid.keys()):\n",
    "    print(f\"{param_name}: {best_parameters[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'vect__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__min_df': 3, 'vect__max_df': 0.6, 'model_lr__solver': 'saga', 'model_lr__C': 10}\n",
      "Best F1 score: 0.773666816402159\n"
     ]
    }
   ],
   "source": [
    "f1_lr = random_search.best_score_\n",
    "model_lr_best = random_search.best_estimator_\n",
    "# Выводим наилучшую комбинацию гиперпараметров и соответствующую ей F1-меру на валидационной выборке\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best F1 score:\", f1_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer(max_df=0.6, min_df=3)),\n",
       "                ('model_lr', LogisticRegression(C=10, solver='saga'))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основное отличие между LinearSVC и LogisticRegression заключается в функции потерь, которую они оптимизируют. LinearSVC использует функцию потерь \"hinge\", которая хорошо работает на задачах с линейно разделимыми классами, но может давать неопределенные результаты на задачах с пересекающимися классами. В то же время, LogisticRegression использует функцию потерь \"logistic\", которая хорошо работает на задачах с пересекающимися классами, но может давать неопределенные результаты на задачах с линейно разделимыми классами.\n",
    "\n",
    "Кроме того, LinearSVC обычно более ресурсоемкий и медленный, чем LogisticRegression, особенно при работе с большими наборами данных. Но при этом он может быть более устойчив к выбросам и шумам в данных, поскольку он оптимизирует границу максимальной ширины (maximum margin).\n",
    "\n",
    "В отличие от LogisticRegression, LinearSVC не имеет параметра регуляризации L1, но вместо этого имеет параметр регуляризации C, который управляет силой регуляризации. Также LinearSVC может использовать различные функции ядра для работы с нелинейными задачами классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                ('model_svc', LinearSVC(random_state=42))])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"vect\", TfidfVectorizer()),\n",
    "        (\"model_svc\", LinearSVC(random_state=42))\n",
    "    ]\n",
    ")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Hyperparameters to be evaluated:\n",
      "{'model_svc__C': [0.1, 0.5, 1, 5, 10],\n",
      " 'vect__max_df': (0.4, 0.6, 0.8, 1.0),\n",
      " 'vect__min_df': (1, 3, 5),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2)),\n",
      " 'vect__norm': ('l1', 'l2')}\n"
     ]
    }
   ],
   "source": [
    "# Задание сетки параметров\n",
    "parameter_grid = {\n",
    "    \"vect__max_df\": (0.4, 0.6, 0.8, 1.0),\n",
    "    \"vect__min_df\": (1, 3, 5),\n",
    "    \"vect__ngram_range\": ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    \"vect__norm\": (\"l1\", \"l2\"),\n",
    "    'model_svc__C': [0.1, 0.5, 1, 5, 10],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=parameter_grid,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    n_iter=12,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"Hyperparameters to be evaluated:\")\n",
    "pprint(parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Done in 868.980s\n",
      "CPU times: user 14min 10s, sys: 16.7 s, total: 14min 27s\n",
      "Wall time: 14min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t0 = time()\n",
    "random_search.fit(X_train_valid, y_train_valid)\n",
    "print(f\"Done in {time() - t0:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters combination found:\n",
      "model_svc__C: 1\n",
      "vect__max_df: 0.6\n",
      "vect__min_df: 1\n",
      "vect__ngram_range: (1, 1)\n",
      "vect__norm: l2\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters combination found:\")\n",
    "best_parameters = random_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameter_grid.keys()):\n",
    "    print(f\"{param_name}: {best_parameters[param_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'vect__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__min_df': 1, 'vect__max_df': 0.6, 'model_svc__C': 1}\n",
      "Best F1 score: 0.7770091704993187\n"
     ]
    }
   ],
   "source": [
    "f1_svc = random_search.best_score_\n",
    "model_svc_best = random_search.best_estimator_\n",
    "# Выводим наилучшую комбинацию гиперпараметров и соответствующую ей F1-меру на валидационной выборке\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best F1 score:\", f1_svc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost на неподготовленных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_valid, df_test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "df_train, df_valid = train_test_split(df_train_valid, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cb = CatBoostClassifier(verbose=100,\n",
    "                              eval_metric='F1',\n",
    "                              random_state=42\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.105251\n",
      "0:\tlearn: 0.7027743\ttest: 0.7481962\tbest: 0.7481962 (0)\ttotal: 517ms\tremaining: 8m 36s\n",
      "100:\tlearn: 0.7435512\ttest: 0.7551900\tbest: 0.7562500 (85)\ttotal: 51s\tremaining: 7m 33s\n",
      "200:\tlearn: 0.7602787\ttest: 0.7609034\tbest: 0.7609034 (191)\ttotal: 1m 39s\tremaining: 6m 36s\n",
      "300:\tlearn: 0.7708763\ttest: 0.7615714\tbest: 0.7637213 (260)\ttotal: 2m 30s\tremaining: 5m 48s\n",
      "400:\tlearn: 0.7797177\ttest: 0.7617567\tbest: 0.7637213 (260)\ttotal: 3m 20s\tremaining: 4m 59s\n",
      "500:\tlearn: 0.7864427\ttest: 0.7630148\tbest: 0.7642718 (463)\ttotal: 4m 11s\tremaining: 4m 10s\n",
      "600:\tlearn: 0.7910320\ttest: 0.7637490\tbest: 0.7654130 (558)\ttotal: 5m 4s\tremaining: 3m 21s\n",
      "700:\tlearn: 0.7955702\ttest: 0.7645922\tbest: 0.7654130 (558)\ttotal: 5m 56s\tremaining: 2m 32s\n",
      "800:\tlearn: 0.7986508\ttest: 0.7625677\tbest: 0.7654130 (558)\ttotal: 6m 49s\tremaining: 1m 41s\n",
      "900:\tlearn: 0.8013135\ttest: 0.7634533\tbest: 0.7654130 (558)\ttotal: 7m 41s\tremaining: 50.8s\n",
      "999:\tlearn: 0.8038012\ttest: 0.7650696\tbest: 0.7654130 (558)\ttotal: 8m 33s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7654129508\n",
      "bestIteration = 558\n",
      "\n",
      "Shrink model to first 559 iterations.\n",
      "CPU times: user 7min 45s, sys: 53.7 s, total: 8min 38s\n",
      "Wall time: 8min 41s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f7a172e5580>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model_cb.fit(df_train[['text']], df_train[['toxic']],\n",
    "         eval_set=(df_valid[['text']], df_valid[['toxic']]),\n",
    "         text_features=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предсказание меток классов для тестовой выборки\n",
    "predictions = model_cb.predict(df_valid[['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.765412950756107\n"
     ]
    }
   ],
   "source": [
    "# вычисление метрики F1 на тестовой выборке\n",
    "f1_cb = f1_score(df_valid[['toxic']], predictions)\n",
    "\n",
    "# вывод результата метрики F1\n",
    "print('F1 score:', f1_cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.280899\n",
      "[2]\tvalid_0's binary_logloss: 0.259441\n",
      "[3]\tvalid_0's binary_logloss: 0.24495\n",
      "[4]\tvalid_0's binary_logloss: 0.233832\n",
      "[5]\tvalid_0's binary_logloss: 0.22502\n",
      "[6]\tvalid_0's binary_logloss: 0.217763\n",
      "[7]\tvalid_0's binary_logloss: 0.212201\n",
      "[8]\tvalid_0's binary_logloss: 0.206687\n",
      "[9]\tvalid_0's binary_logloss: 0.202187\n",
      "[10]\tvalid_0's binary_logloss: 0.19842\n",
      "[11]\tvalid_0's binary_logloss: 0.195053\n",
      "[12]\tvalid_0's binary_logloss: 0.191622\n",
      "[13]\tvalid_0's binary_logloss: 0.188378\n",
      "[14]\tvalid_0's binary_logloss: 0.185514\n",
      "[15]\tvalid_0's binary_logloss: 0.182795\n",
      "[16]\tvalid_0's binary_logloss: 0.179944\n",
      "[17]\tvalid_0's binary_logloss: 0.177864\n",
      "[18]\tvalid_0's binary_logloss: 0.175433\n",
      "[19]\tvalid_0's binary_logloss: 0.173198\n",
      "[20]\tvalid_0's binary_logloss: 0.171044\n",
      "[21]\tvalid_0's binary_logloss: 0.16962\n",
      "[22]\tvalid_0's binary_logloss: 0.167643\n",
      "[23]\tvalid_0's binary_logloss: 0.165948\n",
      "[24]\tvalid_0's binary_logloss: 0.164697\n",
      "[25]\tvalid_0's binary_logloss: 0.16333\n",
      "[26]\tvalid_0's binary_logloss: 0.161899\n",
      "[27]\tvalid_0's binary_logloss: 0.160539\n",
      "[28]\tvalid_0's binary_logloss: 0.159323\n",
      "[29]\tvalid_0's binary_logloss: 0.158009\n",
      "[30]\tvalid_0's binary_logloss: 0.156671\n",
      "[31]\tvalid_0's binary_logloss: 0.155617\n",
      "[32]\tvalid_0's binary_logloss: 0.154443\n",
      "[33]\tvalid_0's binary_logloss: 0.153596\n",
      "[34]\tvalid_0's binary_logloss: 0.152546\n",
      "[35]\tvalid_0's binary_logloss: 0.151575\n",
      "[36]\tvalid_0's binary_logloss: 0.150746\n",
      "[37]\tvalid_0's binary_logloss: 0.149999\n",
      "[38]\tvalid_0's binary_logloss: 0.149342\n",
      "[39]\tvalid_0's binary_logloss: 0.148534\n",
      "[40]\tvalid_0's binary_logloss: 0.147873\n",
      "[41]\tvalid_0's binary_logloss: 0.146999\n",
      "[42]\tvalid_0's binary_logloss: 0.146287\n",
      "[43]\tvalid_0's binary_logloss: 0.14566\n",
      "[44]\tvalid_0's binary_logloss: 0.144958\n",
      "[45]\tvalid_0's binary_logloss: 0.14434\n",
      "[46]\tvalid_0's binary_logloss: 0.143714\n",
      "[47]\tvalid_0's binary_logloss: 0.14312\n",
      "[48]\tvalid_0's binary_logloss: 0.14249\n",
      "[49]\tvalid_0's binary_logloss: 0.141861\n",
      "[50]\tvalid_0's binary_logloss: 0.141222\n",
      "[51]\tvalid_0's binary_logloss: 0.14084\n",
      "[52]\tvalid_0's binary_logloss: 0.140358\n",
      "[53]\tvalid_0's binary_logloss: 0.139837\n",
      "[54]\tvalid_0's binary_logloss: 0.139399\n",
      "[55]\tvalid_0's binary_logloss: 0.139062\n",
      "[56]\tvalid_0's binary_logloss: 0.138563\n",
      "[57]\tvalid_0's binary_logloss: 0.138271\n",
      "[58]\tvalid_0's binary_logloss: 0.137859\n",
      "[59]\tvalid_0's binary_logloss: 0.137499\n",
      "[60]\tvalid_0's binary_logloss: 0.137067\n",
      "[61]\tvalid_0's binary_logloss: 0.13679\n",
      "[62]\tvalid_0's binary_logloss: 0.136302\n",
      "[63]\tvalid_0's binary_logloss: 0.135911\n",
      "[64]\tvalid_0's binary_logloss: 0.135626\n",
      "[65]\tvalid_0's binary_logloss: 0.135303\n",
      "[66]\tvalid_0's binary_logloss: 0.134979\n",
      "[67]\tvalid_0's binary_logloss: 0.134516\n",
      "[68]\tvalid_0's binary_logloss: 0.134156\n",
      "[69]\tvalid_0's binary_logloss: 0.133895\n",
      "[70]\tvalid_0's binary_logloss: 0.133455\n",
      "[71]\tvalid_0's binary_logloss: 0.133256\n",
      "[72]\tvalid_0's binary_logloss: 0.132967\n",
      "[73]\tvalid_0's binary_logloss: 0.132601\n",
      "[74]\tvalid_0's binary_logloss: 0.132317\n",
      "[75]\tvalid_0's binary_logloss: 0.132009\n",
      "[76]\tvalid_0's binary_logloss: 0.131625\n",
      "[77]\tvalid_0's binary_logloss: 0.131458\n",
      "[78]\tvalid_0's binary_logloss: 0.131191\n",
      "[79]\tvalid_0's binary_logloss: 0.131\n",
      "[80]\tvalid_0's binary_logloss: 0.130722\n",
      "[81]\tvalid_0's binary_logloss: 0.130507\n",
      "[82]\tvalid_0's binary_logloss: 0.130324\n",
      "[83]\tvalid_0's binary_logloss: 0.130125\n",
      "[84]\tvalid_0's binary_logloss: 0.129939\n",
      "[85]\tvalid_0's binary_logloss: 0.129639\n",
      "[86]\tvalid_0's binary_logloss: 0.129487\n",
      "[87]\tvalid_0's binary_logloss: 0.129244\n",
      "[88]\tvalid_0's binary_logloss: 0.129106\n",
      "[89]\tvalid_0's binary_logloss: 0.128872\n",
      "[90]\tvalid_0's binary_logloss: 0.128699\n",
      "[91]\tvalid_0's binary_logloss: 0.128498\n",
      "[92]\tvalid_0's binary_logloss: 0.128308\n",
      "[93]\tvalid_0's binary_logloss: 0.12816\n",
      "[94]\tvalid_0's binary_logloss: 0.12799\n",
      "[95]\tvalid_0's binary_logloss: 0.127669\n",
      "[96]\tvalid_0's binary_logloss: 0.127446\n",
      "[97]\tvalid_0's binary_logloss: 0.127323\n",
      "[98]\tvalid_0's binary_logloss: 0.127182\n",
      "[99]\tvalid_0's binary_logloss: 0.127016\n",
      "[100]\tvalid_0's binary_logloss: 0.126869\n",
      "CPU times: user 3min 36s, sys: 305 ms, total: 3min 36s\n",
      "Wall time: 3min 37s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(random_state=42)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Определение параметров модели\n",
    "params = {\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Создание и обучение модели\n",
    "model = LGBMClassifier(**params)\n",
    "model.fit(X_train_tfidf, y_train,\n",
    "          eval_set=[(X_valid_tfidf, y_valid)],\n",
    "          eval_metric='f1',\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-score: 0.7571884984025559\n"
     ]
    }
   ],
   "source": [
    "# Предсказание вероятностей\n",
    "y_pred = model.predict(X_valid_tfidf)\n",
    "\n",
    "# Вычисление значения F1-меры\n",
    "f1_lgb = f1_score(y_valid, y_pred)\n",
    "\n",
    "# Вывод значения метрики на валидационной выборке\n",
    "print('Test F1-score:', f1_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.773667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.777009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.757188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>0.765413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model        f1\n",
       "0  LinearRegression  0.773667\n",
       "1         LinearSVC  0.777009\n",
       "2              LGBM  0.757188\n",
       "3          CatBoost  0.765413"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(columns=['model', 'f1'], \n",
    "             data={\n",
    "                 'model':['LinearRegression', 'LinearSVC', 'LGBM', 'CatBoost'], \n",
    "                 'f1':[f1_lr, f1_svc, f1_lgb, f1_cb]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшая модель: LinearSVC с гиперпараметрами по умолчанию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1-score: 0.7674183776506227\n"
     ]
    }
   ],
   "source": [
    "# Предсказание меток классов на тестовой выборке\n",
    "y_pred = model_svc_best.predict(X_test)\n",
    "\n",
    "# Вычисление значения F1-меры на тестовой выборке\n",
    "f1_svc_test = f1_score(y_test, y_pred)\n",
    "\n",
    "# Вывод значения метрики на тестовой выборке\n",
    "print('Test F1-score:', f1_svc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Требование по минималльному f1=0.75 выполнено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках проекта была разработана модель классификации комментариев на позитивные и негативные с целью автоматической модерации токсичных комментариев в интернет-магазине. Были выполнены все этапы проекта, включая предобработку данных, обучение и тестирование моделей, выбор оптимальной модели и достижение требуемой метрики F1 не меньше 0.75.\n",
    "\n",
    "В дальнейшем, для улучшения качества модели и расширения ее функционала, можно рассмотреть следующие направления развития:\n",
    "\n",
    "1. Улучшение предобработки данных. Возможны дополнительные методы очистки текстов от лишних символов, оптимизация лемматизации, использование более точных алгоритмов векторизации.\n",
    "\n",
    "2. Оптимизация параметров модели. Можно рассмотреть оптимизацию параметров модели для улучшения ее точности и скорости работы. Например, можно провести более широкий поиск по сетке оптимальных параметров модели и применить нейросети.\n",
    "\n",
    "3. Расширение функционала модерации. Можно рассмотреть возможность расширения функционала модерации, включая определение других категорий комментариев, например, спам или оскорбительные высказывания.\n",
    "\n",
    "4. Интеграция модели в рабочее окружение магазина. После успешного тестирования модели необходимо интегрировать ее в рабочее окружение магазина, чтобы модерация комментариев происходила автоматически и эффективно.\n",
    "\n",
    "Таким образом, дальнейшее развитие проекта может быть направлено на улучшение качества модели, расширение ее функционала и интеграцию в рабочее окружение магазина для обеспечения автоматической модерации комментариев."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 5760,
    "start_time": "2023-04-16T12:48:52.204Z"
   },
   {
    "duration": 15,
    "start_time": "2023-04-16T12:49:12.229Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-16T12:49:31.103Z"
   },
   {
    "duration": 2199,
    "start_time": "2023-04-16T12:49:34.937Z"
   },
   {
    "duration": 25,
    "start_time": "2023-04-16T12:49:44.549Z"
   },
   {
    "duration": 27,
    "start_time": "2023-04-16T12:49:45.785Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-16T13:03:03.136Z"
   },
   {
    "duration": 20,
    "start_time": "2023-04-16T13:03:03.142Z"
   },
   {
    "duration": 22,
    "start_time": "2023-04-16T13:03:03.164Z"
   },
   {
    "duration": 5567,
    "start_time": "2023-04-16T13:03:03.188Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-16T13:03:08.757Z"
   },
   {
    "duration": 17,
    "start_time": "2023-04-16T13:03:08.770Z"
   },
   {
    "duration": 1999,
    "start_time": "2023-04-16T13:03:08.788Z"
   },
   {
    "duration": 29,
    "start_time": "2023-04-16T13:03:10.789Z"
   },
   {
    "duration": 25,
    "start_time": "2023-04-16T13:03:10.820Z"
   },
   {
    "duration": 1650,
    "start_time": "2023-04-16T13:03:10.847Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-16T13:03:12.499Z"
   },
   {
    "duration": 13,
    "start_time": "2023-04-16T13:03:12.503Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-16T13:03:12.518Z"
   },
   {
    "duration": 27,
    "start_time": "2023-04-16T13:03:12.534Z"
   },
   {
    "duration": 1222702,
    "start_time": "2023-04-16T13:03:12.563Z"
   },
   {
    "duration": 358,
    "start_time": "2023-04-16T13:23:35.267Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.627Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.628Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.630Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.631Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.633Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.634Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.635Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.636Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.638Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.639Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.641Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.642Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.643Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.644Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.646Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.647Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.648Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.649Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.651Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.652Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.653Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.654Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.656Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.657Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.658Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.712Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.713Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.714Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.715Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.716Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T13:23:35.717Z"
   },
   {
    "duration": 15025,
    "start_time": "2023-04-16T13:29:27.736Z"
   },
   {
    "duration": 40,
    "start_time": "2023-04-16T13:29:42.763Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-16T13:29:42.806Z"
   },
   {
    "duration": 61,
    "start_time": "2023-04-16T13:29:42.813Z"
   },
   {
    "duration": 29,
    "start_time": "2023-04-16T13:29:42.877Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-16T13:29:42.908Z"
   },
   {
    "duration": 9453,
    "start_time": "2023-04-16T13:29:42.916Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-16T13:29:52.371Z"
   },
   {
    "duration": 23,
    "start_time": "2023-04-16T13:29:52.378Z"
   },
   {
    "duration": 59258,
    "start_time": "2023-04-16T13:29:52.404Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-16T13:30:51.665Z"
   },
   {
    "duration": 5016,
    "start_time": "2023-04-16T13:30:51.671Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-16T13:30:56.689Z"
   },
   {
    "duration": 36,
    "start_time": "2023-04-16T13:30:56.697Z"
   },
   {
    "duration": 1411,
    "start_time": "2023-04-16T13:30:56.735Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-16T13:31:48.717Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-16T13:31:48.723Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-16T13:31:48.737Z"
   },
   {
    "duration": 4949,
    "start_time": "2023-04-16T13:31:48.751Z"
   },
   {
    "duration": 12,
    "start_time": "2023-04-16T13:31:53.703Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-16T13:31:53.716Z"
   },
   {
    "duration": 2195,
    "start_time": "2023-04-16T13:31:53.727Z"
   },
   {
    "duration": 23,
    "start_time": "2023-04-16T13:31:55.924Z"
   },
   {
    "duration": 26,
    "start_time": "2023-04-16T13:31:55.949Z"
   },
   {
    "duration": 256,
    "start_time": "2023-04-16T13:31:55.976Z"
   },
   {
    "duration": 2,
    "start_time": "2023-04-16T13:31:56.234Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-16T13:31:56.238Z"
   },
   {
    "duration": 20,
    "start_time": "2023-04-16T13:31:56.245Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-16T13:31:56.268Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-16T15:39:37.604Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-16T15:39:37.609Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-16T15:39:37.615Z"
   },
   {
    "duration": 6003,
    "start_time": "2023-04-16T15:39:37.622Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-16T15:39:43.627Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-16T15:39:43.639Z"
   },
   {
    "duration": 1983,
    "start_time": "2023-04-16T15:39:43.650Z"
   },
   {
    "duration": 20,
    "start_time": "2023-04-16T15:39:45.635Z"
   },
   {
    "duration": 47,
    "start_time": "2023-04-16T15:39:45.656Z"
   },
   {
    "duration": 1639,
    "start_time": "2023-04-16T15:39:45.704Z"
   },
   {
    "duration": 2,
    "start_time": "2023-04-16T15:39:47.345Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-16T15:39:47.349Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-16T15:39:47.359Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-16T15:39:47.368Z"
   },
   {
    "duration": 1325468,
    "start_time": "2023-04-16T15:39:47.375Z"
   },
   {
    "duration": 60,
    "start_time": "2023-04-16T16:01:52.845Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-16T16:01:52.914Z"
   },
   {
    "duration": 38,
    "start_time": "2023-04-16T16:01:52.920Z"
   },
   {
    "duration": 32,
    "start_time": "2023-04-16T16:01:52.959Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-16T16:01:52.994Z"
   },
   {
    "duration": 11191,
    "start_time": "2023-04-16T16:01:53.004Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-16T16:02:04.201Z"
   },
   {
    "duration": 24,
    "start_time": "2023-04-16T16:02:04.214Z"
   },
   {
    "duration": 204901,
    "start_time": "2023-04-16T16:02:04.240Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-16T16:05:29.142Z"
   },
   {
    "duration": 4761,
    "start_time": "2023-04-16T16:05:29.146Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-16T16:05:33.909Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-16T16:05:33.921Z"
   },
   {
    "duration": 1202,
    "start_time": "2023-04-16T16:05:33.934Z"
   },
   {
    "duration": 139945,
    "start_time": "2023-04-16T16:05:35.138Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-16T16:07:55.089Z"
   },
   {
    "duration": 38,
    "start_time": "2023-04-16T16:07:55.099Z"
   },
   {
    "duration": 21,
    "start_time": "2023-04-16T16:07:55.139Z"
   },
   {
    "duration": 62,
    "start_time": "2023-04-16T16:07:55.162Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-16T16:07:55.227Z"
   },
   {
    "duration": 563210,
    "start_time": "2023-04-16T16:07:55.232Z"
   },
   {
    "duration": 422,
    "start_time": "2023-04-16T16:17:18.444Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T16:17:18.867Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T16:17:18.869Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T16:17:18.870Z"
   },
   {
    "duration": 0,
    "start_time": "2023-04-16T16:17:18.871Z"
   },
   {
    "duration": 727,
    "start_time": "2023-04-16T16:22:16.225Z"
   },
   {
    "duration": 27,
    "start_time": "2023-04-16T16:22:26.019Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-16T16:22:42.048Z"
   },
   {
    "duration": 210292,
    "start_time": "2023-04-16T16:23:15.508Z"
   },
   {
    "duration": 1298,
    "start_time": "2023-04-16T16:26:45.804Z"
   },
   {
    "duration": 14,
    "start_time": "2023-04-16T16:26:47.104Z"
   },
   {
    "duration": 1164,
    "start_time": "2023-04-16T16:38:13.114Z"
   },
   {
    "duration": 1143,
    "start_time": "2023-04-16T16:38:22.424Z"
   },
   {
    "duration": 48,
    "start_time": "2023-04-17T08:15:11.024Z"
   },
   {
    "duration": 4884,
    "start_time": "2023-04-17T08:16:28.815Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-17T08:16:33.701Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T08:16:33.712Z"
   },
   {
    "duration": 1870,
    "start_time": "2023-04-17T08:16:33.718Z"
   },
   {
    "duration": 31,
    "start_time": "2023-04-17T08:16:35.589Z"
   },
   {
    "duration": 27,
    "start_time": "2023-04-17T08:16:35.621Z"
   },
   {
    "duration": 1594,
    "start_time": "2023-04-17T08:16:35.650Z"
   },
   {
    "duration": 2,
    "start_time": "2023-04-17T08:16:37.246Z"
   },
   {
    "duration": 18,
    "start_time": "2023-04-17T08:16:37.250Z"
   },
   {
    "duration": 18,
    "start_time": "2023-04-17T08:16:37.270Z"
   },
   {
    "duration": 11,
    "start_time": "2023-04-17T08:16:37.289Z"
   },
   {
    "duration": 1231969,
    "start_time": "2023-04-17T08:16:37.302Z"
   },
   {
    "duration": 51,
    "start_time": "2023-04-17T08:37:09.273Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T08:37:09.326Z"
   },
   {
    "duration": 50,
    "start_time": "2023-04-17T08:37:09.332Z"
   },
   {
    "duration": 31,
    "start_time": "2023-04-17T08:37:09.384Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T08:37:09.417Z"
   },
   {
    "duration": 9292,
    "start_time": "2023-04-17T08:37:09.423Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-17T08:37:18.717Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-17T08:44:13.608Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-17T08:45:01.853Z"
   },
   {
    "duration": 3212,
    "start_time": "2023-04-17T08:49:31.133Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-17T08:51:56.573Z"
   },
   {
    "duration": 1465,
    "start_time": "2023-04-17T08:52:01.073Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-17T10:12:10.916Z"
   },
   {
    "duration": 747,
    "start_time": "2023-04-17T10:12:39.210Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T10:14:27.336Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-17T10:14:43.636Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T10:14:54.214Z"
   },
   {
    "duration": 770,
    "start_time": "2023-04-17T10:15:01.360Z"
   },
   {
    "duration": 1142776,
    "start_time": "2023-04-17T10:15:37.140Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T10:43:44.688Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T10:50:12.900Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T10:58:28.181Z"
   },
   {
    "duration": 19,
    "start_time": "2023-04-17T11:24:22.100Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-17T11:25:39.625Z"
   },
   {
    "duration": 23,
    "start_time": "2023-04-17T11:25:45.109Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-17T11:26:31.414Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T11:26:36.662Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T11:28:52.640Z"
   },
   {
    "duration": 85,
    "start_time": "2023-04-17T11:30:55.976Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-17T11:31:04.198Z"
   },
   {
    "duration": 162926,
    "start_time": "2023-04-17T11:31:12.325Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T11:34:12.426Z"
   },
   {
    "duration": 8,
    "start_time": "2023-04-17T11:46:59.662Z"
   },
   {
    "duration": 320157,
    "start_time": "2023-04-17T11:47:19.931Z"
   },
   {
    "duration": 77,
    "start_time": "2023-04-17T11:55:26.471Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T11:55:39.070Z"
   },
   {
    "duration": 78,
    "start_time": "2023-04-17T11:56:04.125Z"
   },
   {
    "duration": 10,
    "start_time": "2023-04-17T11:56:56.120Z"
   },
   {
    "duration": 834449,
    "start_time": "2023-04-17T11:57:03.603Z"
   },
   {
    "duration": 18,
    "start_time": "2023-04-17T12:18:34.591Z"
   },
   {
    "duration": 3,
    "start_time": "2023-04-17T12:18:50.185Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-17T12:23:56.411Z"
   },
   {
    "duration": 465,
    "start_time": "2023-04-17T12:27:03.111Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-17T12:27:15.101Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T12:31:26.137Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-17T12:31:43.280Z"
   },
   {
    "duration": 6,
    "start_time": "2023-04-17T13:12:21.423Z"
   },
   {
    "duration": 7,
    "start_time": "2023-04-17T13:18:01.606Z"
   },
   {
    "duration": 868984,
    "start_time": "2023-04-17T13:18:36.913Z"
   },
   {
    "duration": 5,
    "start_time": "2023-04-17T13:33:23.140Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T13:33:47.021Z"
   },
   {
    "duration": 513,
    "start_time": "2023-04-17T13:35:18.858Z"
   },
   {
    "duration": 59,
    "start_time": "2023-04-17T13:49:41.309Z"
   },
   {
    "duration": 4,
    "start_time": "2023-04-17T13:49:41.370Z"
   },
   {
    "duration": 521679,
    "start_time": "2023-04-17T13:49:41.376Z"
   },
   {
    "duration": 627,
    "start_time": "2023-04-17T13:58:23.056Z"
   },
   {
    "duration": 9,
    "start_time": "2023-04-17T13:58:23.685Z"
   },
   {
    "duration": 217998,
    "start_time": "2023-04-17T13:58:23.696Z"
   },
   {
    "duration": 1334,
    "start_time": "2023-04-17T14:02:01.712Z"
   },
   {
    "duration": 13,
    "start_time": "2023-04-17T14:02:03.048Z"
   },
   {
    "duration": 642,
    "start_time": "2023-04-17T14:02:03.064Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "184.549px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
